<!DOCTYPE html>
<html>
  <head>
    <title>BayesComp Satellite Workshop on Bayesian Computation and Inference with Misspecified Models</title>
    
    <link rel="stylesheet" type="text/css" href="css/main.css">
  
  </head>

  <body>
    
        <nav>
<ul>
<!-- <li><a href="/">Home</a></li> -->
<!-- <li><a href="/registration">Registration</a></li> -->
<!-- <li><a href="/deadlines">Deadlines</a></li> -->
<!-- <li><a href="/submission">Submission</a></li> -->
<!-- <li><a href="/programme">Programme</a></li> -->
<!-- <li><a href="/award">Award</a></li> -->
<!-- <li><a href="/past">Past</a></li> -->
<!-- <li><a href="/accommodation">Accommodation</a></li> -->
<!-- <li><a href="/travel">Travel</a></li> -->
<!-- <li><a href="/organisation">Organisation</a></li> -->
</ul>
    </nav>
    

    
  <div class="container">

		<div class="blurb">
			<h1>Bayesian Computation and Inference with Misspecified Models <img src="images/BayesCompHeader.png" alt="BayesComp poster", width="460", height="183", align="right", style="padding:30px;"></h1>
			<h4>Satellite Workshop BayesComp 2025</h4>
			<p><strong>Date:</strong> 16-17 June 2025.</p> 
			<p><strong>Location:</strong> National University of Singapore.</p>
			
		</div> 

<div class="blurb">

	
<p>
This <a href="https://bayescomp2025.sg/">BayesComp 2025</a> Satellite Workshop is about methods, theory and applications for Bayesian inference with misspecified models.
</p>

<section>
  
<h4> Outline </h4>

<p>  
A common justification for the use of Bayesian inference is that Bayes’ theorem is the optimal way to update beliefs based on new observations, and that representing beliefs through a posterior distribution is desirable for uncertainty quantification. However, standard posterior distributions are only meaningful when the model or likelihood is well-specified, which is not the case in the presence of outliers, adversarial contaminations, or faulty measurement instruments. This realisation has led to an increased focus on generalisations of Bayesian inference which aim to produce ‘generalised posterior distributions’ that provide some representation of uncertainty but also overcome some of the lack of robustness of standard posteriors. The aim of this workshop will be to give a broad overview of this topic, touching on both foundational questions and algorithmic advances, and inviting the BayesComp community to take a more active role in solving some of the remaining open challenges in this area.
</p>

</div>

<div class="blurb">
<!--  
<h4> Call for Poster Sessions Abstracts </h4>
<p> We would like to invite the submission of abstracts for the poster session on the second afternoon. Abstracts can be submitted <a href="https://forms.gle/PtXz5YR5iLNiAULJ7">here</a> and should be limited to 200 words. The deadline for abstract submission is 1st May 2025. A selection of accepted posters will be given the opportunity to present their work through a talk in the spotlight session.
</p>-->
  
  
</div>


<div class="blurb">

<h3> Programme </h3>
<p>The tentative programme for this workshop is below. This programme is subject to change.
</p>

	
<h4> Monday 16 June </h4>
<p>
<ul>
<li>09.00-10.30 Tutorial: <a href="https://jeremiasknoblauch.github.io/"> Jeremias Knoblauch </a> - Post-Bayesian inference</li>
<li>10.30-11.00 <b>Coffee</b></li>
<li>11.00-11.45 Invited Talk:  </li>
<li>11.45-12.30 Invited Talk: <a href="https://jeremiehoussineau.com/"> Jeremie Houssineau </a> - Robust Bayesian inference with possibility theory </li>
<li>12.30-14.00 <b>Lunch</b></li>
<li>14.00-15.30 Contributed Talks </li>
<ul>
	<li> Ayush Bharti </li>
	<li> Chris Oates </li>	
	<li> Adam Bretheron </li>
	<li> Yann McLatchie </li>	
</ul>
<li>15.30-16.00 <b>Coffee</b></li>
<li>16.00-17.00 Invited Talk: <a href="https://search.usi.ch/en/people/f8960de6d60dd08a79b6c1eb20b7442b/mira-antonietta"> Antonietta Mira </a>  - Likelihood distortion and Bayesian local robustness </li>
<li>17.00-18.00 Invited Talk: <a href="https://dtfrazier.netlify.app/"> David Frazier </a> - Data-centric semi-modular Bayesian inference </li>
</ul>
</p>

<h4> Tuesday 17 June </h4>
<p>
<ul>
<li>09.00-09.45 Invited Talk: <a href="https://eweinstein.github.io/"> Eli Weinstein </a>  - Adaptive nonparametric perturbations of parametric Bayesian models</li>
<li>09.45-10.30 Invited Talk: <a href="https://www.maths.ed.ac.uk/~ateckent/"> Aretha Teckentrup </a>  - Misspecification in Gaussian process regression</li>
<li>10.30-11.00 <b>Coffee</b></li>
<li>11.00-11.45 Invited Talk: <a href="http://jhhuggins.org"> Jonathan Huggins </a>  - Resampling within MCMC: Approaches, computational benefits, and statistical properties </li>
<li>11.45-12.30 Invited Talk: <a href="https://haritadell.github.io/"> Harita Dellaporta </a>  - Model-based distributionally robust optimisation: Bayesian ambiguity sets and model misspecification </li>
<li>12.30-14.00 <b>Lunch</b></li>
<li>14.00-15.30 Contributed Talks </li>
<ul>
	<li> Matias Altamirano </li>
	<li> Ryan Kelly </li>
	<li> Roubing Tang </li>
	<li> Gerardo Duran Martin</li>	
</ul>
<li>15.30-16.00 <b>Coffee</b></li>
<li>16.00-17.00 Invited Talk: <a href="https://pierrealquier.github.io/"> Pierre Alquier </a>  - A new mutual information bound for statistical inference</li>
<li>17.00-18.00 Invited Talk: <a href="https://edfong.github.io/"> Edwin Fong </a>  - Model misspecification in martingale posteriors </li>
</ul>
</p>	
	
</div>

<div class="blurb">

	<h4> Abstracts - Invited Talks </h4>

	<ul>

		<li> <strong><a href="https://jeremiasknoblauch.github.io/"> Jeremias Knoblauch (University College London, UK).</a></strong> <br/> <br/>

			
			<strong> Title: </strong> Post-Bayesian inference. <br/>

			<strong> Abstract: </strong> In this talk, I provide my perspective on our  community's efforts to develop inference procedures with Bayesian characteristics that go beyond Bayes' Rule as an epistemological principle.  I will explain why these efforts are needed, as well as the forms which they take. Focusing on some of my own contributions to the field, I will trace out some of the community's most important milestones, as well as the challenges that lie ahead. Throughout, I will provide success stories of the field, and emphasise the new opportunities that open themselves up to us once we dare to go beyond orthodox Bayesian procedures. <br/>
		</li>

		<li> <strong><a href="https://search.usi.ch/en/people/f8960de6d60dd08a79b6c1eb20b7442b/mira-antonietta"> Antonietta Mira (Università
della Svizzera italiana, Switzerland).</a></strong> <br/><br/>

		
			<strong> Title: </strong> Likelihood distortion and Bayesian local robustness. <br/>

			<strong> Abstract: </strong> Robust Bayesian analysis has primarily focused on detecting and measuring robustness with respect to the prior distribution. Much of the existing literature aims to define suitable classes of priors that allow one to compute variations in quantities of interest as the prior changes within those classes. By contrast, the robustness of Bayesian methods with respect to the likelihood function has received far less attention, mainly due to mathematical and computational complexities, and because the likelihood is often perceived as a more objective choice than the prior. In this work, we propose a new approach to Bayesian local robustness, focusing mainly  on robustness with respect to the likelihood function. We then extend the approach to account for robustness with respect to the prior alone and, finally, to both the prior and the likelihood simultaneously. Our method builds on the notion of a distortion function from risk theory. The resulting robustness measure is a local sensitivity measure that proves to be highly tractable and straightforward to compute for various classes of distortion functions. We derive its asymptotic properties and provide numerical experiments to illustrate the theory and demonstrate its practical utility in modeling applications. Joint work with A. Di Noia  and F. Ruggeri. <br/>
    
			

		</li>
		
		<li> <strong><a href="https://jeremiehoussineau.com/"> Jeremie Houssineau (Nanyang Technological University, Singapore).</a></strong> <br/><br/>

		
			<strong> Title:  </strong> Robust Bayesian inference with possibility theory.  <br/>

			<strong> Abstract:  </strong> In Bayesian inference, the marginal likelihood is one of the main quantities for model selection based on its ability to capture the fitness of the model. Yet, when reformulating Bayesian inference in the context of possibility theory, the marginal likelihood no longer favours simpler models and cannot be used directly for model selection. What it can do however, is capture the coherence between the model and the observation(s). This possibilistic marginal likelihood therefore provides a simple solution to make the underlying inference more robust, leveraging useful properties that are specific to possibility theory. Illustrations will be given for multiple problems of varying difficulty, focusing on robustness to outliers.</strong>  <br/>


		</li>
			

		</li>
		
		<li> <strong><a href="https://dtfrazier.netlify.app/"> David Frazier (Monash University, Australia).</a></strong> <br/><br/>

			

			<strong> Title: </strong> Data-centric semi-modular Bayesian inference. <br/>

			<strong> Abstract: </strong> Modular Bayesian methods perform inference in models that are specified through a collection of coupled sub-models, known as modules. These modules often arise from modeling different data sources or from combining domain knowledge from different disciplines. “Cutting feedback” is a Bayesian inference method that ensures misspecification of one module does not affect inferences for parameters in other modules, and produces what is known as the cut posterior. However, choosing between the cut posterior and the standard Bayesian posterior is challenging. When misspecification is not severe, cutting feedback can greatly increase posterior uncertainty without a large reduction of estimation bias, leading to a bias-variance trade-off. This tradeoff motivates semi-modular posteriors, which interpolate between standard and cut posteriors based on a tuning parameter. This talk reviews recent results on the accuracy of semi-modular posteriors, and details situations and conditions under which semi-modular posteriors are more accurate than standard posteriors and cut posteriors. We empirically illustrate the improvements in inferential accuracy capable under semi-modular methods in linear regression models and copula models. <br/>


			

		</li>
		
		<li> <strong><a href="https://eweinstein.github.io/"> Eli Weinstein (Columbia University, US).</a></strong> <br/><br/>

			
			
			<strong> Title: </strong> Adaptive nonparametric perturbations of parametric Bayesian models. <br/>

			<strong> Abstract: </strong> Parametric Bayesian modeling offers a powerful and flexible toolbox for scientific data analysis. Yet the model, however detailed, may still be wrong, and this can make inferences untrustworthy. In this talk we study nonparametrically perturbed parametric (NPP) Bayesian models, in which a parametric Bayesian model is relaxed via a distortion of its likelihood. We analyze the properties of NPP models when the target of inference is the true data distribution or some functional of it, such as in causal inference. We show that NPP models can offer the robustness of nonparametric models while retaining the data efficiency of parametric models, achieving fast convergence when the parametric model is close to true. To efficiently analyze data with an NPP model, we develop a generalized Bayes procedure to approximate its posterior. We demonstrate our method by estimating causal effects of gene expression from single cell RNA sequencing data. NPP modeling offers an efficient approach to robust Bayesian inference and can be used to robustify any parametric Bayesian model. <br/>
      

			

		</li>
		
		<li> <strong><a href="https://www.maths.ed.ac.uk/~ateckent/"> Aretha Teckentrup (University of Edinburgh, UK).</a></strong> <br/><br/>

			

			<strong> Title: </strong> Misspecification in Gaussian process regression. <br/>

			<strong> Abstract: </strong> Gaussian process (GP) regression is often used as a Bayesian inference procedure to recover an unknown function from observed (noisy) point values. In practice, hyper-parameters appearing in the mean and covariance structure of the Gaussian process prior, such as smoothness of the function and typical length scales, are often unknown and learnt from the data, along with the posterior mean and covariance. In this talk, we are interested in the convergence of GP regression as the number of training points goes to infinity, and in particular how the possibly misspecified choice of hyper-parameters affects the convergence. Using results from scattered data approximation, we provide a convergence analysis of the method applied to a fixed, unknown function of interest.
 <br/>


			

		</li>
		
		<li> <strong><a href="https://edfong.github.io/"> Edwin Fong (University of Hong Kong, Hong Kong).</a></strong> <br/><br/>

			
			<strong> Title: </strong> Model misspecification in martingale posteriors. <br/>

			<strong> Abstract: </strong> The martingale posterior is a framework where posterior uncertainty is directly generated through predictive imputation. The Bayesian model can then be directly specified using a sequence of predictive distributions, bypassing the need for explicit likelihood and prior specifications. Given the absence of a likelihood however, it is not immediately clear what the definition or impact of misspecification is for martingale posteriors. This talk will investigate predictive notions of misspecification, the consequences they have on parameter inference, and potential remedies. <br/>

			

		</li>
		
		<li> <strong><a href="https://haritadell.github.io/"> Harita Dellaporta (University College London, UK).</a></strong> <br/><br/>

			
			<strong> Title: </strong> Model-based distributionally robust optimisation: Bayesian ambiguity sets and model misspecification.   <br/>

			<strong> Abstract: </strong> Decision making under uncertainty is challenging as the data-generating process (DGP) is often unknown. Bayesian inference proceeds by estimating the DGP through posterior beliefs about the model's parameters. However, minimising the expected risk under these posterior beliefs can lead to sub-optimal decisions due to model uncertainty or limited, noisy observations. This talk will address this problem by introducing Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DRO-BAS) which hedges against model uncertainty by optimising the worst-case risk over a posterior-informed ambiguity set.  Simulations show DRO-BAS Pareto dominates existing Bayesian DRO formulations when evaluating the out-of-sample mean-variance trade-off, while achieving faster solve times. However, when the model is misspecified, this can lead to over-conservative decisions as the DGP might not be contained anymore in the ambiguity set. I will briefly discuss how Bayesian Ambiguity Sets can be easily adjusted to address this challenge by introducing DRO with Robust, to model misspecification, Bayesian Ambiguity Sets. These are expected Maximum Mean Discrepancy ambiguity sets under a robust posterior that incorporates beliefs about the DGP. The resulting optimisation problem obtains a dual formulation in the Reproducing Kernel Hilbert Space for any choice of model family.   <br/>

			

		</li>
		
		<li> <strong><a href="https://pierrealquier.github.io/"> Pierre Alquier (ESSEC Business School, Singapore).</a></strong> <br/><br/>

			
			<strong> Title: </strong> A new mutual information bound for statistical inference. <br/>

			<strong> Abstract: </strong> Recent advances in statistical learning theory have revealed profound connections between mutual information (MI) bounds, PAC-Bayesian theory, and Bayesian nonparametrics. This work introduces a novel mutual information bound for statistical inference. The derived bound has wide-ranging applications. It yields improved contraction rates for fractional posteriors in Bayesian nonparametrics. It can also be used to study variational inference or Maximum Likelihood Estimation (MLE). By bridging these diverse areas, this work advances our understanding of the fundamental limits of statist ical inference and the role of information in learning from data. We hope that these results will not only clarify connections between statistical inference and information theory but also help to develop a new toolbox to study a wide range of estimators. <br/>

			

		</li>

	</ul>

</div>



<div class="blurb">


<h4>Organisation</h4>


	<ul>
			<li> <strong><a href="https://fxbriol.github.io/"> François-Xavier Briol</a></strong>, UCL (f.briol@ucl.ac.uk) </li>

			<li> <strong><a href="https://sites.google.com/view/jack-jewson-academic-profile/home"> Jack Jewson</a></strong>, Monash University (jack.jewson@monash.edu)</li>

			<li> <strong><a href="https://jeremiasknoblauch.github.io/"> Jeremias Knoblauch</a></strong>, UCL (j.knoblauch@ucl.ac.uk)</li>
	</ul>


</div>

</div>

  <footer>
   <!-- <img src="https://steinworkshop.github.io/ati.jpg" alt="Ati Logo", width="255", height="115", align="center", style="padding:10px;"> -->
	<!-- Some logos here -->
  </footer>

</body>
</html>
